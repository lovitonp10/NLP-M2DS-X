{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB movie reviews with Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We retrieve the textual data in the variable *texts*.\n",
    "\n",
    "The labels are retrieved in the variable $y$ - it contains *len(texts)* of them: $0$ indicates that the corresponding review is negative while $1$ indicates that it is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_labels = np.ones(len(test_texts), dtype=np.int)\\ntest_labels[:len(test_texts_neg)] = 0.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "# We get the files from the path: ./aclImdb/train/neg for negative reviews, and ./aclImdb/train/pos for positive reviews\n",
    "train_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'train', 'neg', '*.txt')))\n",
    "train_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'train', 'pos', '*.txt')))\n",
    "\n",
    "\"\"\"\n",
    "test_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'test', 'neg', '*.txt')))\n",
    "test_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'test', 'pos', '*.txt')))\n",
    "\"\"\"\n",
    "\n",
    "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
    "train_texts_neg = [open(f, encoding=\"utf8\").read() for f in train_filenames_neg]\n",
    "train_texts_pos = [open(f, encoding=\"utf8\").read() for f in train_filenames_pos]\n",
    "train_texts = train_texts_neg + train_texts_pos\n",
    "\n",
    "\"\"\"\n",
    "test_texts_neg = [open(f, encoding=\"utf8\").read() for f in test_filenames_neg]\n",
    "test_texts_pos = [open(f, encoding=\"utf8\").read() for f in test_filenames_pos]\n",
    "test_texts = test_texts_neg + test_texts_pos\n",
    "\"\"\"\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "train_labels = np.ones(len(train_texts), dtype=int)\n",
    "train_labels[:len(train_texts_neg)] = 0.\n",
    "\n",
    "\"\"\"\n",
    "test_labels = np.ones(len(test_texts), dtype=np.int)\n",
    "test_labels[:len(test_texts_neg)] = 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './aclImdb/train/neg/0_3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/q7nn178n5cldx0lvl6yvbbn40000gn/T/ipykernel_35183/2692713009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./aclImdb/train/neg/0_3.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './aclImdb/train/neg/0_3.txt'"
     ]
    }
   ],
   "source": [
    "open(\"./aclImdb/train/neg/0_3.txt\", encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, the impact of our choice of representations upon our results will also depend on the quantity of data we use:** try to see how changing the parameter ```k``` affects our results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2500\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 10\n",
    "train_texts_reduced = train_texts[0::k]\n",
    "train_labels_reduced = train_labels[0::k]\n",
    "\n",
    "print('Number of documents:', len(train_texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a function from sklearn, ```train_test_split```, to separate data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_texts_splt, val_texts, train_labels_splt, val_labels = train_test_split(train_texts_reduced, train_labels_reduced, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted representation of documents\n",
    "\n",
    "Our statistical model, like most models applied to textual data, uses counts of word occurrences in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, containing the counts of each word (regardless of their order of occurrence) in the document. \n",
    "\n",
    "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
    "\n",
    "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
    "\n",
    "We can use the **CountVectorizer** class from scikit-learn to obtain these representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 2, 1, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the list containing the words ordered according to their index (Note that words of 2 characters or less are not counted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text: get the BoW representations ##\n",
    "\n",
    "The first thing to do is to turn the review from a string into a list of words. The simplest method is to divide the string according to spaces with the command:\n",
    "``text.split()``\n",
    "\n",
    "But we must also be careful to remove special characters that may not have been cleaned up (such as HTML tags if the data was obtained from web pages). Since we're going to count words, we'll have to build a list of tokens appearing in our data. In our case, we'd like to reduce this list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
    "We will therefore use a function adapted to our needs - but this is a job that we generally don't need to do ourselves, since there are many tools already adapted to most situations. \n",
    "For text cleansing, there are many scripts, based on different tools (regular expressions, for example) that allow you to prepare data. The division of the text into words and the management of punctuation is handled in a step called *tokenization*; if needed, a python package like NLTK contains many different *tokenizers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
      "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
     ]
    }
   ],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove small words (1 and 2 characters)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens\n",
    "\n",
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
    "print(clean_and_tokenize(corpus_raw))\n",
    "print(word_tokenize(corpus_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function takes as input a list of documents (each in the form of a string) and returns, as in the example using ``CountVectorizer``:\n",
    "- A vocabulary that associates, to each word encountered, an index\n",
    "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
    "\n",
    "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    n_samples = len(texts)\n",
    "    if voc == None:\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words = words.union(set(clean_and_tokenize(text))) # list of all words\n",
    "        n_features = len(words) # number of different words\n",
    "        vocabulary = dict(zip(words, range(n_features))) # vocab[wd] = index ; indexation\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "        n_features = len(vocabulary)\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    for k, text in enumerate(texts): # énumeration: renvoie (k, texts[k]) \n",
    "        for w in clean_and_tokenize(text):\n",
    "            if w in vocabulary:\n",
    "                counts[k][vocabulary[w]] += 1.\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'walk': 0, 'city': 1, 'boulevard': 2, 'avenue': 3, 'down': 4, 'the': 5, 'ran': 6, 'walked': 7}\n",
      "[[0. 0. 1. 0. 2. 1. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 2. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Voc, X = count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful: check the memory that the representations are going to use (given the way they are build). What ```CountVectorizer``` argument allows to avoid the issue ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 29596)\n"
     ]
    }
   ],
   "source": [
    "voc, train_bow = count_words(train_texts)\n",
    "print(train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29596)\n"
     ]
    }
   ],
   "source": [
    "_, val_bow = count_words(val_texts, voc)\n",
    "print(val_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 25601)\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer(max_features=30000)\n",
    "Bow = vectorizer.fit_transform(train_texts_splt)\n",
    "train_bow = Bow.toarray()\n",
    "print(train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25601)\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation data\n",
    "val_bow = vectorizer.transform(val_texts).toarray()\n",
    "print(val_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian \n",
    "\n",
    "### Main idea\n",
    "\n",
    "A movie review is in fact a list of words $s = (w_1, ..., w_N)$, and we try to find the associated class $c$ - which in our case may be $c = 0$ or $c = 1$. The objective is thus to find for each review $s$ the class $\\hat{c}$ maximizing the conditional probability **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothesis : P(s) is constant for each class** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Naive hypothesis : the variables (words) of a review are independant between themselves** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General view\n",
    "\n",
    "#### Training: Estimating the probabilities\n",
    "\n",
    "For each word $w$ in the vocabulary $V$, $P(w|c)$ is the number of occurrences of $w$ in all reviews of class $c$, divided by the total number of occurrences in $c$. If we note $T(w,c)$ this number of occurrences, we get:\n",
    "\n",
    "$$P(w|c) = \\text{Frequency of }w\\text{ in }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
    "\n",
    "#### Test: Calculating scores\n",
    "\n",
    "To facilitate the calculations and to avoid *underflow* and approximation errors, we use the log-sum trick, and we pass the equation into log-probabilities : \n",
    "\n",
    "$$ \\hat{c} = \\underset{c}{\\mathrm{argmax}} P(c|s) = \\underset{c}{\\mathrm{argmax}} \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right] $$\n",
    "\n",
    "#### Laplace smoothing\n",
    "\n",
    "A word that does not appear in a document has a probability of zero: this will cause issues with the logarithm. So we keep a very small part of the probability mass that we redistribute with the *Laplace smoothing*: \n",
    "\n",
    "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} (T(w',c) + 1)}$$\n",
    "\n",
    "There are other smoothing methods, generally suitable for other, more complex applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail: training\n",
    "\n",
    "The idea is to extract the number of occurrences $T(w,c)$ for each word $w$ and each class $c$, which will make it possible to calculate the matrix of conditional probabilities $\\pmb{P}$ such that: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Note that the number of occurrences $T(w,c)$ can be easily obtained from the BoW representations of all documents !\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "- Extract the vocabulary $V$ and counts $T(w,c)$ for each of the words $w$ and classes $c$, from a set of documents.\n",
    "- Calculate the a priori probabilities of the classes $P(c) = \\frac{\\sum_{w \\in V} T(w,c)}{\\sum_{c \\in C} \\sum_{w \\in V} T(w,c)}$\n",
    "- Calculate the conditional **smoothed** probabilities $P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$.\n",
    "\n",
    "## Detail: test\n",
    "\n",
    "We now know the conditional probabilities given by the $\\pmb{P}$ matrix. \n",
    "Now we must obtain $P(s|c)$ for the current document. This quantity is obtained using a simple calculation involving the BoW representation of the document and $\\pmb{P}$.\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "- For each of the classes $c$,\n",
    "    - $Score(c) = \\log P(c)$\n",
    "    - For each word $w$ in the document to be tested:\n",
    "        - $Score(c) += \\log P(w|c)$\n",
    "- Return $argmax_{c \\in C} Score(c)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore be able to use the reviews at our disposal to **estimate the probabilities $P(w|c)$ for each word $w$ given the two classes $c$**. These reviews will allow us to learn how to evaluate the \"compatibility\" between words and classes.\n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Training**: will learn a statistical model based on the representations $X$ corresponding to the labels $y$.\n",
    "Here, $X$ contains representations obtained as the output of ```count_words```. You can complete the function using the procedure detailed above. \n",
    "\n",
    "Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\alpha$, which we can implement as an argument of the class \"NB\".\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing**: will return the labels predicted by the model for other representations $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha est un paramètre pour le lissage (smoothing)\n",
    "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        classes = np.unique(y) # all labels\n",
    "        n_classes = len(classes) # number of labels\n",
    "\n",
    "        prior = np.zeros(n_classes) # probabilités des classes à priori\n",
    "\n",
    "        tct = np.zeros((n_classes, n_features))\n",
    "        for k, c in enumerate(classes):\n",
    "            prior[k] = np.sum(y == c) / float(n_samples) # a priori, ce sont les fréquences\n",
    "            tct[k, :] = np.sum(X[y == c, :], axis=0) # nombre(words|classe)\n",
    "\n",
    "        alpha = self.alpha  # laplace smoothing / lissage de laplace\n",
    "        cond_prob = (tct + alpha) / np.sum(tct + alpha, axis=1)[:, None]\n",
    "\n",
    "        self.prior_ = prior\n",
    "        self.log_cond_prob_ = np.log(cond_prob) # On enregistre les probabilités conditionelles dans un attribut de la classe (avec self)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = len(self.prior_)\n",
    "        scores = np.dot(X, self.log_cond_prob_.T) # On se sert de l'attribut appris dans \"fit\"\n",
    "        scores += np.log(self.prior_)[None, :]\n",
    "        return np.array([0, 1])[np.argmax(scores, axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nb = NB()\n",
    "nb.fit(train_bow, train_labels_splt)\n",
    "val_pred = nb.predict(val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/q7nn178n5cldx0lvl6yvbbn40000gn/T/ipykernel_14317/294897230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_pred))\n",
    "cm = confusion_matrix(val_labels , val_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(train_bow, train_labels_splt)\n",
    "val_pred = clf_nb.predict(val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       250\n",
      "           1       0.81      0.76      0.78       250\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.79      0.79      0.79       500\n",
      "weighted avg       0.79      0.79      0.79       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_pred))\n",
    "cm = confusion_matrix(val_labels , val_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
